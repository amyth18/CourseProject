{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"emails.csv\", index_col=\"Unnamed: 0\")\n",
    "#df.info()\n",
    "Number_of_Topics = df.shape[0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ameet/work/pycharm/cs410/final_project/CourseProject/source/venv/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    }
   ],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(ngram_range=(1,2),stop_words=stopwords, min_df=1, max_df=0.9)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ameet/work/pycharm/cs410/final_project/CourseProject/source/venv/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:294: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_text_model = NMF(n_components=Number_of_Topics, random_state=42)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, features, no_top_words=3):\n",
    "    topics_list = []\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        str2 = \"Topic %02d\" % topic\n",
    "        #print(\"\\nTopic %02d\" % topic)\n",
    "        #print(\"\\n\", str2)\n",
    "        str1 = \"\"\n",
    "        for i in range(0, no_top_words):\n",
    "            str2 = str(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))\n",
    "            print(str2)\n",
    "            str1 = str1 + str2\n",
    "        topics_list.append(str1)\n",
    "    return topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  activity (3.42)\n",
      "  access (2.59)\n",
      "  google (2.44)\n",
      "  read (1.48)\n",
      "  tech (1.06)\n",
      "  min read (1.06)\n",
      "  taj (1.85)\n",
      "  brand (0.93)\n",
      "  hotel (0.93)\n",
      "  travel (1.38)\n",
      "  tripadvisor (1.26)\n",
      "  travellers (1.16)\n",
      "  google (3.68)\n",
      "  account (1.40)\n",
      "  new account (1.35)\n",
      "  fund (1.75)\n",
      "  nfo (1.73)\n",
      "  taiwan (1.48)\n",
      "  wazirx (3.66)\n",
      "  trading (1.99)\n",
      "  zero (1.99)\n",
      "  data (1.49)\n",
      "  min (1.43)\n",
      "  data science (0.67)\n",
      "  citibank (2.20)\n",
      "  help bank (1.57)\n",
      "  bank (1.39)\n",
      "  novotel (4.17)\n",
      "  novotel goa (3.98)\n",
      "  dona (3.78)\n",
      "  cloud (3.01)\n",
      "  free (2.96)\n",
      "  google cloud (2.87)\n",
      "  weta (0.59)\n",
      "  unity (0.55)\n",
      "  techcrunch (0.42)\n",
      "  rs (1.23)\n",
      "  price (0.74)\n",
      "  et (0.61)\n",
      "  app (0.98)\n",
      "  live (0.98)\n",
      "  markets (0.98)\n",
      "  hdfc (3.15)\n",
      "  hdfc mutual (2.29)\n",
      "  mutual (2.03)\n",
      "  news (3.84)\n",
      "  sources (2.08)\n",
      "  google news (1.56)\n",
      "  destinations (1.14)\n",
      "  experiences (1.14)\n",
      "  taj (1.05)\n",
      "  innercircle (1.76)\n",
      "  taj (1.59)\n",
      "  taj innercircle (1.54)\n",
      "  changes (2.87)\n",
      "  changes review (2.22)\n",
      "  suspect locked (2.22)\n",
      "  access (3.27)\n",
      "  google (2.70)\n",
      "  apps (2.38)\n",
      "  tripadvisor (2.65)\n",
      "  bengaluru (1.57)\n",
      "  restaurants (1.35)\n",
      "  password (3.26)\n",
      "  changed (3.04)\n",
      "  google (2.90)\n",
      "  read (1.46)\n",
      "  min read (0.99)\n",
      "  min (0.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ameet/work/pycharm/cs410/final_project/CourseProject/source/venv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#tfidf_text_vectorizer.get_feature_names()\n",
    "all_topics = display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.11729751, 3.18550487, 3.01559608, 4.6819436 , 2.18977506,\n",
       "       2.91200478, 4.07438733, 3.87301655, 2.5267187 , 4.34278666,\n",
       "       3.9977564 , 4.8795175 , 4.61359092, 4.6786314 , 5.13978605,\n",
       "       4.45422522, 4.96702775, 5.51668049, 5.32331304, 4.56935137,\n",
       "       4.43401928, 5.32212482, 5.18494461])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How big are the topics. ie How many documents could be assigned to each topic\n",
    "# this is normalized\n",
    "W_text_matrix.sum(axis=0)/W_text_matrix.sum()*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add topic to dataframe\n",
    "i = 0\n",
    "df[\"NMF_topic\"] = \"\"\n",
    "for r in W_text_matrix:\n",
    "    loc = np.where(r == np.amax(r))[0][0]\n",
    "    df.iat[i, df.columns.get_loc('NMF_topic')]=all_topics[loc]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_text_model = TruncatedSVD(n_components = Number_of_Topics, random_state=42)\n",
    "W_svd_text_matrix = svd_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_svd_text_matrix = svd_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  google (2.05)\n",
      "  access (1.20)\n",
      "  activity (1.20)\n",
      "  read (0.58)\n",
      "  min (0.45)\n",
      "  min read (0.38)\n",
      "  taj (1.14)\n",
      "  tripadvisor (1.02)\n",
      "  restaurants (0.54)\n",
      "  tripadvisor (15.04)\n",
      "  travellers (7.80)\n",
      "  travel (6.58)\n",
      "  google (1.12)\n",
      "  news (0.79)\n",
      "  cloud (0.58)\n",
      "  google (5.37)\n",
      "  news (3.18)\n",
      "  cloud (2.81)\n",
      "  wazirx (25.51)\n",
      "  trading (16.32)\n",
      "  zero (14.12)\n",
      "  data (0.83)\n",
      "  min (0.50)\n",
      "  weta (0.49)\n",
      "  novotel (4.18)\n",
      "  novotel goa (3.98)\n",
      "  goa dona (3.78)\n",
      "  novotel (6.87)\n",
      "  novotel goa (6.54)\n",
      "  goa dona (6.21)\n",
      "  wazirx (8.49)\n",
      "  data (5.52)\n",
      "  citibank (5.15)\n",
      "  weta (9.14)\n",
      "  hdfc (8.86)\n",
      "  unity (8.48)\n",
      "  cloud (4.16)\n",
      "  free (4.07)\n",
      "  google cloud (3.98)\n",
      "  free (6.99)\n",
      "  cloud (6.96)\n",
      "  google cloud (6.80)\n",
      "  hdfc (28.52)\n",
      "  hdfc mutual (20.74)\n",
      "  mutual (16.43)\n",
      "  apps (22.89)\n",
      "  google (18.25)\n",
      "  account (14.45)\n",
      "  experiences (6.06)\n",
      "  destinations (6.06)\n",
      "  designed (5.19)\n",
      "  innercircle (4.56)\n",
      "  taj innercircle (3.99)\n",
      "  15 savings (2.85)\n",
      "  accountyou (17.27)\n",
      "  review protect (17.21)\n",
      "  accountif (17.21)\n",
      "  access (8.43)\n",
      "  apps (4.85)\n",
      "  travel (4.77)\n",
      "  travel (11.52)\n",
      "  travellers (6.11)\n",
      "  suppliers (6.04)\n",
      "  password (29.42)\n",
      "  changed (27.31)\n",
      "  password google (18.20)\n",
      "  tigerchops (12.39)\n",
      "  im (10.84)\n",
      "  remote (10.62)\n"
     ]
    }
   ],
   "source": [
    "all_topics = display_topics(svd_text_model, tfidf_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add topic to dataframe\n",
    "i = 0\n",
    "df[\"SVD_topic\"] = \"\"\n",
    "for r in W_svd_text_matrix:\n",
    "    loc = np.where(r == np.amax(r))[0][0]\n",
    "    df.iat[i, df.columns.get_loc('SVD_topic')]=all_topics[loc]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ameet/work/pycharm/cs410/final_project/CourseProject/source/venv/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_text_vectorizer = CountVectorizer(ngram_range=(1,2), stop_words=stopwords, min_df=1, max_df=0.9)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_text_model = LatentDirichletAllocation(n_components = Number_of_Topics, random_state=42)\n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  free (2.61)\n",
      "  cloud (2.07)\n",
      "  google (1.96)\n",
      "  taj (0.83)\n",
      "  destinations (0.73)\n",
      "  experiences (0.73)\n",
      "  zone medium (0.01)\n",
      "  finish google (0.01)\n",
      "  find answers (0.01)\n",
      "  travel (1.10)\n",
      "  tripadvisor (1.06)\n",
      "  travellers (1.01)\n",
      "  2021 (1.25)\n",
      "  wazirx (1.25)\n",
      "  november (0.77)\n",
      "  zone medium (0.01)\n",
      "  finish google (0.01)\n",
      "  find answers (0.01)\n",
      "  zone medium (0.01)\n",
      "  finish google (0.01)\n",
      "  find answers (0.01)\n",
      "  followfull (0.34)\n",
      "  newsgooglecomwelcome google (0.34)\n",
      "  communities followfull (0.01)\n",
      "  read (1.66)\n",
      "  tech (0.99)\n",
      "  min read (0.90)\n",
      "  zone medium (0.01)\n",
      "  finish google (0.01)\n",
      "  find answers (0.01)\n",
      "  taj (1.52)\n",
      "  hotel (0.76)\n",
      "  brand (0.76)\n",
      "  zone medium (0.01)\n",
      "  finish google (0.01)\n",
      "  find answers (0.01)\n",
      "  zone medium (0.01)\n",
      "  finish google (0.01)\n",
      "  find answers (0.01)\n",
      "  google (2.53)\n",
      "  account (1.36)\n",
      "  access (1.33)\n",
      "  novotel (2.48)\n",
      "  goa (2.36)\n",
      "  novotel goa (2.36)\n",
      "  tripadvisor (1.16)\n",
      "  hdfc (0.95)\n",
      "  mutual (0.69)\n",
      "  rs (1.00)\n",
      "  price (0.60)\n",
      "  buy (0.60)\n",
      "  app (0.67)\n",
      "  live (0.53)\n",
      "  markets (0.53)\n",
      "  weta (0.51)\n",
      "  company (0.47)\n",
      "  unity (0.47)\n",
      "  google (2.93)\n",
      "  account (1.24)\n",
      "  new (0.93)\n",
      "  zone medium (0.01)\n",
      "  finish google (0.01)\n",
      "  find answers (0.01)\n",
      "  zone medium (0.01)\n",
      "  finish google (0.01)\n",
      "  find answers (0.01)\n",
      "  data (0.93)\n",
      "  min (0.89)\n",
      "  science (0.52)\n"
     ]
    }
   ],
   "source": [
    "all_topics = display_topics(lda_text_model, count_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add topic to dataframe\n",
    "i = 0\n",
    "df[\"LDA_topic\"] = \"\"\n",
    "for r in W_lda_text_matrix:\n",
    "    loc = np.where(r == np.amax(r))[0][0]\n",
    "    df.iat[i, df.columns.get_loc('LDA_topic')]=all_topics[loc]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"email_topics.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
